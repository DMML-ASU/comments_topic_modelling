{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from os import listdir\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "# load the data into panda dataframe\n",
    "data_file_name = \"dataset/reviews_Musical_Instruments.json\"\n",
    "#raw_df = pd.read_json(data_file_name, lines=True)\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('commentdata.pkl', 'rb') as handle:\n",
    "    list_of_comments = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "#Dimensionality of the resulting word vectors\n",
    "num_features = 500\n",
    "\n",
    "#Minimum word count threshold\n",
    "min_word_count = 3\n",
    "\n",
    "#Number of threads to run in parallel\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "#Context window length\n",
    "context_size = 4\n",
    "\n",
    "#Seed for the RNG, to make the result reproducible\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath='train/pos'\n",
    "mypath1='train/neg'\n",
    "documents = [f for f in listdir(mypath) if f.endswith('.txt')]\n",
    "documents1 = [f for f in listdir(mypath1) if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath='unsup/'\n",
    "def getMovieDat():\n",
    "   \n",
    "   #documents = [f for f in listdir(mypath) if f.startswith('data_')]\n",
    "   #documents = [os.listdir(mypath)]\n",
    "   documents = [f for f in listdir(mypath) if f.endswith('.txt')]\n",
    "   lmtzr = WordNetLemmatizer()\n",
    "   docs=[]\n",
    "   sent_corp=defaultdict(list)\n",
    "   tokenizer = RegexpTokenizer(r'\\w+')\n",
    "   wrd_freq=Counter()\n",
    "   for doc in tqdm(documents):\n",
    "       with open(mypath+doc,'r', errors = 'ignore') as fp:\n",
    "           lines = ' '.join(fp.readlines())\n",
    "           sent = sent_tokenize(lines)\n",
    "           tmp = ''\n",
    "           for sen in sent:\n",
    "               tokens=tokenizer.tokenize(sen)\n",
    "               tagged = nltk.pos_tag(tokens)\n",
    "               nouns = [word for word,pos in tagged \\\n",
    "                        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS'\\\n",
    "                            or pos == 'NNPS' or pos=='JJ')]\n",
    "               downcased = [x.lower() for x in nouns]\n",
    "               downcased = [lmtzr.lemmatize(x) for x in downcased]\n",
    "               downcased = [x for x in downcased if x not in stopwords.words('english')]\n",
    "               if len(downcased)>1:\n",
    "                   sent_corp[doc].append([set(downcased)]+[sen])\n",
    "                   tmp += ' '.join(downcased)\n",
    "                   for x in downcased: wrd_freq[x]+=1\n",
    "           docs.append(tmp)\n",
    "   tp_wrds=sorted(wrd_freq.items(), key=itemgetter(1), reverse=True)\n",
    "   return (docs,tp_wrds,sent_corp)\n",
    "docs,tp_wrds,sent_corp = getMovieDat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'errors' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a4bd7ac067d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmypath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'errors' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "docs=[]\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for doc in tqdm(documents):\n",
    "    with open(mypath+doc,'r', errors = 'ignore') as fp:\n",
    "        lines = ' '.join(fp.readlines())\n",
    "        sent = sent_tokenize(lines)\n",
    "        tmp = ''\n",
    "        for sen in sent:\n",
    "            tokens=tokenizer.tokenize(sen)\n",
    "            downcased = [x.lower() for x in tokens]\n",
    "            tmp.append(downcased)\n",
    "        docs.append(tmp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for i in range(len(docs)):\n",
    "    appended_list = [] \n",
    "    for video in docs[i]:\n",
    "        for comment in video:\n",
    "            appended_list.append(comment)\n",
    "    final_list.append(appended_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmmlasu/.local/lib/python2.7/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "doc2vec_model = gensim.models.doc2vec.Doc2Vec(\n",
    "    seed=seed,\n",
    "    workers=num_workers, \n",
    "    size=num_features, \n",
    "    min_count=min_word_count, \n",
    "    window=context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [gensim.models.doc2vec.TaggedDocument(s,[i]) for i,s in enumerate(final_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2238"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabword2vec_model.iterulary is built\n",
      "Word2Vec vocabulary length:  82338\n"
     ]
    }
   ],
   "source": [
    "doc2vec_model.build_vocab(documents)\n",
    "print(\"The vocabword2vec_model.iterulary is built\")\n",
    "print(\"Word2Vec vocabulary length: \", len(doc2vec_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmmlasu/.local/lib/python2.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "#Start training the model\n",
    "doc2vec_model.train(documents=documents,total_examples=doc2vec_model.corpus_count,epochs=doc2vec_model.iter)\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "#Save the model\n",
    "doc2vec_model.save(\"youtube_data1.d2v\")\n",
    "print(\"Model saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
