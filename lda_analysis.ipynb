{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "import csv\n",
    "import string\n",
    "#nltk.download('popular')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from os import listdir\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import lda\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "mypath='dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2266/2266 [28:02<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "def getMovieDat():\n",
    "    \n",
    "    #documents = [f for f in listdir(mypath) if f.startswith('data_')]\n",
    "    #documents = [os.listdir(mypath)]\n",
    "    data={}\n",
    "    with open('commentsfile1.csv','r', errors = 'ignore') as data_file:\n",
    "        reader=csv.DictReader(data_file)\n",
    "        for row in reader:\n",
    "            if row['video_id'] in data:\n",
    "                data[row['video_id']].append(row['comment_text'].encode(\"ascii\", errors=\"ignore\").decode())\n",
    "            else:\n",
    "                data[row['video_id']] = [row['comment_text']]\n",
    "    #documents = [f for f in listdir(mypath) if f.endswith('.txt')]\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    docs=[]\n",
    "    sent_corp=defaultdict(list)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    wrd_freq=Counter()\n",
    "    for key in tqdm(data):\n",
    "        lines = ' '\n",
    "        for e in data[key]:\n",
    "            lines = lines + e\n",
    "        sent = sent_tokenize(lines)\n",
    "        tmp = ''\n",
    "        for sen in sent:\n",
    "            tokens=tokenizer.tokenize(sen)\n",
    "            tagged = nltk.pos_tag(tokens)\n",
    "            nouns = [word for word,pos in tagged \\\n",
    "                     if (pos == 'NN' or pos == 'NNP' or pos == 'NNS'\\\n",
    "                         or pos == 'NNPS' or pos=='JJ' and len(word)>2)]\n",
    "            downcased = [x.lower() for x in nouns]\n",
    "            downcased = [lmtzr.lemmatize(x) for x in downcased]\n",
    "            downcased = [x for x in downcased if x not in stopwords.words('english')]\n",
    "            if len(downcased)>1:\n",
    "                sent_corp[key].append([set(downcased)]+[sen])\n",
    "                tmp += ' '.join(downcased)\n",
    "                for x in downcased: wrd_freq[x]+=1\n",
    "        docs.append(tmp)\n",
    "        \n",
    "    temp_docs = []# [docs[i].replace(j,'') for i in docs  for j,v in wrd_freq if j in i and v==1]\n",
    "    for i, doc in enumerate(docs):\n",
    "        for j in wrd_freq:\n",
    "            nj = ' ' + j + ' '\n",
    "            if nj in doc and wrd_freq[j]==1:\n",
    "                try:\n",
    "                    d = temp_docs[i]\n",
    "                except:\n",
    "                    d = doc\n",
    "\n",
    "                temp = d.replace(nj, ' ')\n",
    "                \n",
    "                try:\n",
    "                    temp_docs[i] = temp\n",
    "                except:\n",
    "                    temp_docs.append(temp)\n",
    "                \n",
    "    tp_wrds=sorted(wrd_freq.items(), key=itemgetter(1), reverse=True)\n",
    "    return (temp_docs,tp_wrds,sent_corp)\n",
    "docs,tp_wrds,sent_corp = getMovieDat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block converts words to integers for LDA input\n",
    "def getMovNAmazonDat(docs):\n",
    "     \n",
    "    # change the location of data here\n",
    "    print(\"Function called!!\")\n",
    "    vectorizer = CountVectorizer(min_df=1,stop_words='english')\n",
    "    for d in docs:\n",
    "        if not (d.strip()):\n",
    "            docs.remove(d)\n",
    "    reditposts=docs[:]\n",
    "    rv_vec=vectorizer.fit_transform(reditposts)\n",
    "    f_nam=tuple(vectorizer.get_feature_names()) \n",
    "    print(\"Vectorize done!!\")\n",
    "    return(rv_vec,f_nam,docs)\n",
    "\n",
    "train_d,f_nam,docs = getMovNAmazonDat(docs)\n",
    "print(\"Function over!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the traditional LDA model\n",
    "model = lda.LDA(n_topics=100, n_iter=1000, random_state=1)\n",
    "model.fit(train_d)\n",
    "topic_word = model.topic_word_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.doc_topic_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "topic_doc = pd.DataFrame(model.doc_topic_)\n",
    "maxprob = []\n",
    "maxprob = topic_doc.apply(lambda x: x.argmax(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# qry=set(['screen','brightness','display','lcd','reflectiveness'])\n",
    "qry=set(['logan','apology','phone','parody','artist'])\n",
    "n_top_words = 100\n",
    "top_word_list = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(f_nam)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    tmp=set(topic_words)\n",
    "    #if qry.intersection(tmp):\n",
    "     #   print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "    top_word_list.append('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result=[]\n",
    "for i in maxprob:\n",
    "    result.append(top_word_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[1]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
